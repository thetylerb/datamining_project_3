{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "999b2ab2",
   "metadata": {},
   "source": [
    "\n",
    "# DM Project 3 — Regression (Starter Notebook)\n",
    "\n",
    "This notebook scaffolds a clean, reproducible workflow for the Ames Housing regression task (Kaggle‑style).  \n",
    "Update paths as needed to point to your local `dm_project_3/` folder that already contains `train.csv` and `test.csv`.\n",
    "\n",
    "**Workflow overview**\n",
    "1. Load data & sanity checks  \n",
    "2. Minimal EDA (shapes, nulls, distributions)  \n",
    "3. Preprocessing (numeric/ordinal/categorical) + feature engineering  \n",
    "4. **Experiments** (at least three):  \n",
    "   - **Exp A**: Baseline Linear Regression on engineered features  \n",
    "   - **Exp B**: Regularized models (Ridge & Lasso) with CV  \n",
    "   - **Exp C**: Gradient Boosting (e.g., HistGradientBoostingRegressor)  \n",
    "5. Compare via cross‑validated RMSE (log‑target option included)  \n",
    "6. Train best model on full train; predict `test.csv`; write `submission.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96e33387",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running in a fresh environment, you may need:\n",
    "# pip install pandas numpy scikit-learn matplotlib\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths (edit if your notebook sits elsewhere)\n",
    "DATA_DIR = Path('dm_project_3')\n",
    "TRAIN_PATH = DATA_DIR / 'train.csv'\n",
    "TEST_PATH  = DATA_DIR / 'test.csv'\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c307bd83",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train = pd.read_csv(\u001b[43mtrain\u001b[49m.csv)\n\u001b[32m      2\u001b[39m test  = pd.read_csv(test.csv)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(train.shape, test.shape)\n",
      "\u001b[31mNameError\u001b[39m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "train = pd.read_csv(train.csv)\n",
    "test  = pd.read_csv(test.csv)\n",
    "\n",
    "print(train.shape, test.shape)\n",
    "display(train.head(3))\n",
    "display(train.describe(include='all').T.head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76df81af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic nulls overview\n",
    "nulls = train.isna().sum().sort_values(ascending=False)\n",
    "display(nulls.head(20))\n",
    "\n",
    "# Target\n",
    "assert 'SalePrice' in train.columns, \"train.csv must have 'SalePrice' column\"\n",
    "train['log_SalePrice'] = np.log1p(train['SalePrice'])  # option for log target\n",
    "\n",
    "fig = plt.figure()\n",
    "train['SalePrice'].hist(bins=50)\n",
    "plt.title('SalePrice distribution')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "train['log_SalePrice'].hist(bins=50)\n",
    "plt.title('log1p(SalePrice) distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5ced1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example feature engineering tailored to Ames:\n",
    "# - TotalSF = GrLivArea + TotalBsmtSF\n",
    "# - Age features: HouseAge, Remodeled (boolean), SinceRemodel\n",
    "# - TotalBaths (Full + Half*0.5 + Basement baths)\n",
    "# - QualityScore (e.g., OverallQual + KitchenQual/ExterQual ordinal maps)\n",
    "#\n",
    "# You can add/remove features in the 'featurize' function.\n",
    "\n",
    "# Ordinal maps for selected quality columns (higher is better)\n",
    "qual_map = {'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}\n",
    "exterqual_cols = ['ExterQual','KitchenQual','HeatingQC']\n",
    "\n",
    "def featurize(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    # Safe fills\n",
    "    for c in ['TotalBsmtSF','GrLivArea','FullBath','HalfBath','BsmtFullBath','BsmtHalfBath']:\n",
    "        if c in out.columns:\n",
    "            out[c] = out[c].fillna(0)\n",
    "    # TotalSF\n",
    "    if set(['GrLivArea','TotalBsmtSF']).issubset(out.columns):\n",
    "        out['TotalSF'] = out['GrLivArea'] + out['TotalBsmtSF']\n",
    "    # Age features\n",
    "    if 'YearBuilt' in out.columns and 'YrSold' in out.columns:\n",
    "        out['HouseAge'] = out['YrSold'] - out['YearBuilt']\n",
    "    if 'YearRemodAdd' in out.columns and 'YrSold' in out.columns:\n",
    "        out['SinceRemodel'] = out['YrSold'] - out['YearRemodAdd']\n",
    "        out['Remodeled'] = (out['YearRemodAdd'] != out['YearBuilt']).astype(int)\n",
    "    # Baths\n",
    "    for col in ['FullBath','HalfBath','BsmtFullBath','BsmtHalfBath']:\n",
    "        if col not in out.columns:\n",
    "            out[col] = 0\n",
    "    out['TotalBaths'] = out['FullBath'] + 0.5*out['HalfBath'] +                         out['BsmtFullBath'] + 0.5*out['BsmtHalfBath']\n",
    "    # QualityScore\n",
    "    for c in exterqual_cols:\n",
    "        if c in out.columns:\n",
    "            out[c] = out[c].map(qual_map).fillna(0)\n",
    "    if set(['OverallQual','KitchenQual','ExterQual']).issubset(out.columns):\n",
    "        out['QualityScore'] = out['OverallQual'] + out['KitchenQual'] + out['ExterQual']\n",
    "    return out\n",
    "\n",
    "# Wrap for sklearn ColumnTransformer with FunctionTransformer\n",
    "featurizer = FunctionTransformer(featurize, feature_names_out='one-to-one')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab506c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identify column types\n",
    "target = 'SalePrice'\n",
    "drop_cols = [target] if target in train.columns else []\n",
    "X_all = train.drop(columns=drop_cols, errors='ignore')\n",
    "y_all = train[target]\n",
    "\n",
    "# After featurize we will recompute columns, so we infer types on raw X\n",
    "numeric_cols = [c for c in X_all.columns if pd.api.types.is_numeric_dtype(X_all[c])]\n",
    "# Selected ordinal quality columns already mapped in featurize; treat as numeric afterwards\n",
    "\n",
    "# Choose a small, interpretable categorical set for baseline\n",
    "categorical_cols = [c for c in X_all.columns \n",
    "                    if (X_all[c].dtype == 'object') and c not in exterqual_cols]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('impute', SimpleImputer(strategy='median')),\n",
    "    ('scale', StandardScaler(with_mean=False))  # sparse-safe\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=True))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799808ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "rmse_scorer = make_scorer(lambda yt, yp: rmse(yt, yp), greater_is_better=False)\n",
    "\n",
    "# Toggle to run models on log-target; predictions will be expm1-transformed for metrics\n",
    "USE_LOG_TARGET = True\n",
    "\n",
    "def y_transform(y):\n",
    "    return np.log1p(y) if USE_LOG_TARGET else y\n",
    "\n",
    "def y_inverse(yhat):\n",
    "    return np.expm1(yhat) if USE_LOG_TARGET else yhat\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80f8c1f",
   "metadata": {},
   "source": [
    "## Experiment A — Baseline OLS (LinearRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc5c102",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_A = Pipeline(steps=[\n",
    "    ('featurize', featurizer),\n",
    "    ('preprocess', preprocess),\n",
    "    ('reg', LinearRegression())\n",
    "])\n",
    "\n",
    "scores_A = []\n",
    "for tr_idx, te_idx in cv.split(train):\n",
    "    X_tr, X_te = train.iloc[tr_idx].copy(), train.iloc[te_idx].copy()\n",
    "    y_tr, y_te = y_transform(y_all.iloc[tr_idx].values), y_all.iloc[te_idx].values\n",
    "\n",
    "    model_A.fit(X_tr, y_tr)\n",
    "    preds = y_inverse(model_A.predict(X_te))\n",
    "    scores_A.append(rmse(y_te, preds))\n",
    "\n",
    "print(\"Exp A (OLS) CV RMSE:\", np.mean(scores_A), \"+/-\", np.std(scores_A))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727be5c0",
   "metadata": {},
   "source": [
    "## Experiment B — Ridge & Lasso (with simple alpha grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d11a731",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alphas = [0.1, 0.5, 1.0, 3.0, 10.0, 30.0, 100.0]\n",
    "results_B = []\n",
    "\n",
    "for reg_name, reg in [('Ridge', Ridge), ('Lasso', Lasso)]:\n",
    "    for a in alphas:\n",
    "        pipe = Pipeline(steps=[\n",
    "            ('featurize', featurizer),\n",
    "            ('preprocess', preprocess),\n",
    "            ('reg', reg(alpha=a, random_state=SEED) if reg_name=='Lasso' else reg(alpha=a))\n",
    "        ])\n",
    "        fold_scores = []\n",
    "        for tr_idx, te_idx in cv.split(train):\n",
    "            X_tr, X_te = train.iloc[tr_idx].copy(), train.iloc[te_idx].copy()\n",
    "            y_tr, y_te = y_transform(y_all.iloc[tr_idx].values), y_all.iloc[te_idx].values\n",
    "            pipe.fit(X_tr, y_tr)\n",
    "            preds = y_inverse(pipe.predict(X_te))\n",
    "            fold_scores.append(rmse(y_te, preds))\n",
    "        results_B.append((reg_name, a, np.mean(fold_scores), np.std(fold_scores)))\n",
    "\n",
    "results_B_df = pd.DataFrame(results_B, columns=['Model','alpha','CV_RMSE','CV_STD']).sort_values('CV_RMSE')\n",
    "display(results_B_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac29bcd4",
   "metadata": {},
   "source": [
    "## Experiment C — Tree‑based: HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e66622",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use numeric-encoded features only for HGB to keep things fast: use OHE too via preprocess\n",
    "model_C = Pipeline(steps=[\n",
    "    ('featurize', featurizer),\n",
    "    ('preprocess', preprocess),\n",
    "    ('reg', HistGradientBoostingRegressor(random_state=SEED))\n",
    "])\n",
    "\n",
    "scores_C = []\n",
    "for tr_idx, te_idx in cv.split(train):\n",
    "    X_tr, X_te = train.iloc[tr_idx].copy(), train.iloc[te_idx].copy()\n",
    "    y_tr, y_te = y_transform(y_all.iloc[tr_idx].values), y_all.iloc[te_idx].values\n",
    "    model_C.fit(X_tr, y_tr)\n",
    "    preds = y_inverse(model_C.predict(X_te))\n",
    "    scores_C.append(rmse(y_te, preds))\n",
    "\n",
    "print(\"Exp C (HGB) CV RMSE:\", np.mean(scores_C), \"+/-\", np.std(scores_C))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d084b9",
   "metadata": {},
   "source": [
    "## Pick the best model (manually for now) and refit on full train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0204fe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: choose the best mean CV RMSE from the experiments above.\n",
    "# For reproducibility, you can paste the best params here.\n",
    "best_model = model_C  # <-- change to model_A or a specific Ridge/Lasso from Experiment B\n",
    "\n",
    "# Fit on FULL training data\n",
    "best_model.fit(train, y_transform(y_all.values))\n",
    "\n",
    "# Create Kaggle submission on test.csv\n",
    "test_pred = y_inverse(best_model.predict(test))\n",
    "sub = pd.DataFrame({'Id': test['Id'], 'SalePrice': test_pred})\n",
    "sub_path = DATA_DIR / 'submission.csv'\n",
    "sub.to_csv(sub_path, index=False)\n",
    "sub_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892b6679",
   "metadata": {},
   "source": [
    "## Optional: Local train/validation split (skip Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2f692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If you don't want to submit to Kaggle, you can evaluate with a final holdout:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(train, y_all, test_size=0.2, random_state=SEED)\n",
    "best_model.fit(X_tr, y_transform(y_tr.values))\n",
    "preds = y_inverse(best_model.predict(X_te))\n",
    "print(\"Holdout RMSE:\", rmse(y_te.values, preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb572d70",
   "metadata": {},
   "source": [
    "\n",
    "### Notes & Next Steps\n",
    "- Try more engineered features (e.g., `TotalPorchSF`, `IsNew`, neighborhood medians, interaction terms).\n",
    "- Replace `HistGradientBoostingRegressor` with `XGBRegressor` or `LGBMRegressor` if allowed in your environment.\n",
    "- Tune hyperparameters using `GridSearchCV` or `RandomizedSearchCV`.\n",
    "- Document each experiment: what changed, why, and the results.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": "fantasy-hockey"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
